---

title: Train a SMILES language model from scratch


keywords: fastai
sidebar: home_sidebar

summary: "Tutorial how to train a reaction language model"
description: "Tutorial how to train a reaction language model"
nb_path: "nbs/08_training_smiles_language_model_from_scratch.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/08_training_smiles_language_model_from_scratch.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">rxnfp.models</span> <span class="kn">import</span> <span class="n">SmilesLanguageModelingModel</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Track-the-training">Track the training<a class="anchor-link" href="#Track-the-training"> </a></h2><p>We will be using wandb to keep track of our training. You can use the an account on <a href="https://www.wandb.com">wandb</a> or create an own instance following the instruction in the <a href="https://docs.wandb.com/self-hosted">documentation</a>.</p>
<p>If you then create an <code>.env</code> file in the root folder and specify the <code>WANDB_API_KEY=</code> (and the <code>WANDB_BASE_URL=</code>), you can use dotenv to load those enviroment variables.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !pip install python-dotenv</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup-MLM-training">Setup MLM training<a class="anchor-link" href="#Setup-MLM-training"> </a></h2><p>Choose the hyperparameters you want and start the training. The default parameters will train a BERT model with 12 layers and 4 attention heads per layer. The training task is Masked Language Modeling (MLM), where tokens from the input reactions are randomly masked and predicted by the model given the context.</p>
<p>After defining the config, the training is launched in 3 lines of code using our adapter written for the  <a href="https://simpletransformers.ai">SimpleTransformers</a> library (based on huggingface <a href="https://github.com/huggingface/transformers">Transformers</a>).</p>
<p>To make it work you will have to install simpletransformers:</p>
<div class="highlight"><pre><span></span>pip install simpletransformers
</pre></div>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;architectures&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&quot;BertForMaskedLM&quot;</span>
  <span class="p">],</span>
  <span class="s2">&quot;attention_probs_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="s2">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
  <span class="s2">&quot;hidden_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
  <span class="s2">&quot;initializer_range&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
  <span class="s2">&quot;intermediate_size&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
  <span class="s2">&quot;layer_norm_eps&quot;</span><span class="p">:</span> <span class="mf">1e-12</span><span class="p">,</span>
  <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
  <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;bert&quot;</span><span class="p">,</span>
  <span class="s2">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
  <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
  <span class="s2">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="s2">&quot;type_vocab_size&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">vocab_path</span> <span class="o">=</span> <span class="s1">&#39;../data/uspto_1k_TPL/individual_files/vocab.txt&#39;</span>

<span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">,</span> 
        <span class="s1">&#39;vocab_path&#39;</span><span class="p">:</span> <span class="n">vocab_path</span><span class="p">,</span> 
        <span class="s1">&#39;wandb_project&#39;</span><span class="p">:</span> <span class="s1">&#39;uspto_mlm_temp_1000&#39;</span><span class="p">,</span>
        <span class="s1">&#39;train_batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s1">&#39;manual_seed&#39;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
        <span class="s2">&quot;fp16&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;num_train_epochs&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s1">&#39;max_seq_length&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s1">&#39;evaluate_during_training&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;overwrite_output_dir&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;output_dir&#39;</span><span class="p">:</span> <span class="s1">&#39;../out/bert_mlm_1k_tpl&#39;</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">1e-4</span>
       <span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SmilesLanguageModelingModel</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;bert&#39;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !unzip ../data/uspto_1k_TPL/individual_files/mlm_training.zip -d ../data/uspto_1k_TPL/individual_files/</span>
<span class="n">train_file</span> <span class="o">=</span> <span class="s1">&#39;../data/uspto_1k_TPL/individual_files/mlm_train_file.txt&#39;</span>
<span class="n">eval_file</span> <span class="o">=</span> <span class="s1">&#39;../data/uspto_1k_TPL/individual_files/mlm_eval_file_1k.txt&#39;</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">train_file</span><span class="o">=</span><span class="n">train_file</span><span class="p">,</span> <span class="n">eval_file</span><span class="o">=</span><span class="n">eval_file</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

