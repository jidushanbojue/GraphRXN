{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapters for SmilesTransformer models\n",
    "\n",
    "> Adapting SmilesTransformer models to use a SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import sklearn\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig, BertForMaskedLM, AlbertConfig, AlbertForMaskedLM\n",
    ")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    wandb_available = True\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "\n",
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try: \n",
    "    import simpletransformers\n",
    "    # The original results were obtained with simpletransformers==0.34.4 and transformers==2.11.0\")\n",
    "except ImportError:\n",
    "    raise ImportError('To use this extension, please install simpletransformers ( \"pip install simpletransformers==0.61.13\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmilesLanguageModelingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# optional\n",
    "from simpletransformers.config.global_args import global_args\n",
    "from simpletransformers.language_modeling import (\n",
    "    LanguageModelingModel\n",
    "\n",
    ")    \n",
    "\n",
    "class SmilesLanguageModelingModel(LanguageModelingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        model_name,\n",
    "        generator_name=None,\n",
    "        discriminator_name=None,\n",
    "        train_files=None,\n",
    "        args=None,\n",
    "        use_cuda=True,\n",
    "        cuda_device=-1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a LanguageModelingModel.\n",
    "        Main difference to https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/classification/classification_model.py\n",
    "        is that it uses a SmilesTokenizer instead of the original Tokenizer.\n",
    "        Args:\n",
    "            model_type: The type of model bert (other model types could be implemented)\n",
    "            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).\n",
    "            generator_name (optional): A pretrained model name or path to a directory containing an ELECTRA generator model.\n",
    "            discriminator_name (optional): A pretrained model name or path to a directory containing an ELECTRA discriminator model.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            train_files (optional): List of files to be used when training the tokenizer.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "        \n",
    "        MODEL_CLASSES = {\n",
    "            \"bert\": (BertConfig, BertForMaskedLM, SmilesTokenizer),\n",
    "            \"albert\": (AlbertConfig, AlbertForMaskedLM, SmilesTokenizer)\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "        self.args = self._load_model_args(model_name)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, LanguageModelingArgs):\n",
    "            self.args = args\n",
    "\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            self.is_sweeping = True\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = sweep_config_to_sweep_values(sweep_config)\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "        else:\n",
    "            self.is_sweeping = False\n",
    "\n",
    "        if self.args.manual_seed:\n",
    "            random.seed(self.args.manual_seed)\n",
    "            np.random.seed(self.args.manual_seed)\n",
    "            torch.manual_seed(self.args.manual_seed)\n",
    "            if self.args.n_gpu > 0:\n",
    "                torch.cuda.manual_seed_all(self.args.manual_seed)\n",
    "\n",
    "        if self.args.local_rank != -1:\n",
    "            logger.info(f\"local_rank: {self.args.local_rank}\")\n",
    "            torch.distributed.init_process_group(backend=\"nccl\")\n",
    "            cuda_device = self.args.local_rank\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \" Make sure CUDA is available or set use_cuda=False.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args.fp16 = False\n",
    "\n",
    "        self.args.model_name = model_name\n",
    "        self.args.model_type = model_type\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        self.tokenizer_class = tokenizer_class\n",
    "        new_tokenizer = False\n",
    "        \n",
    "        if self.args.vocab_path:\n",
    "            self.tokenizer = tokenizer_class(self.args.vocab_path, do_lower_case=False)\n",
    "        elif self.args.tokenizer_name:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(self.args.tokenizer_name, cache_dir=self.args.cache_dir)\n",
    "        elif self.args.model_name:\n",
    "            if self.args.model_name == \"electra\":\n",
    "                self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                    generator_name, cache_dir=self.args.cache_dir, **kwargs\n",
    "                )\n",
    "                self.args.tokenizer_name = self.args.model_name\n",
    "            else:\n",
    "                self.tokenizer = tokenizer_class.from_pretrained(model_name, cache_dir=self.args.cache_dir, **kwargs)\n",
    "                self.args.tokenizer_name = self.args.model_name\n",
    "        else:\n",
    "            if not train_files:\n",
    "                raise ValueError(\n",
    "                    \"model_name and tokenizer_name are not specified.\"\n",
    "                    \"You must specify train_files to train a Tokenizer.\"\n",
    "                )\n",
    "            else:\n",
    "                self.train_tokenizer(train_files)\n",
    "                new_tokenizer = True\n",
    "\n",
    "        if self.args.config_name:\n",
    "            self.config = config_class.from_pretrained(self.args.config_name, cache_dir=self.args.cache_dir)\n",
    "        elif self.args.model_name and self.args.model_name != \"electra\":\n",
    "            self.config = config_class.from_pretrained(model_name, cache_dir=self.args.cache_dir, **kwargs)\n",
    "        else:\n",
    "            self.config = config_class(**self.args.config, **kwargs)\n",
    "        if self.args.vocab_size:\n",
    "            self.config.vocab_size = self.args.vocab_size\n",
    "        if new_tokenizer:\n",
    "            self.config.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        if self.args.model_type == \"electra\":\n",
    "            if generator_name:\n",
    "                self.generator_config = ElectraConfig.from_pretrained(generator_name)\n",
    "            elif self.args.model_name:\n",
    "                self.generator_config = ElectraConfig.from_pretrained(\n",
    "                    os.path.join(self.args.model_name, \"generator_config\"), **kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.generator_config = ElectraConfig(**self.args.generator_config, **kwargs)\n",
    "                if new_tokenizer:\n",
    "                    self.generator_config.vocab_size = len(self.tokenizer)\n",
    "\n",
    "            if discriminator_name:\n",
    "                self.discriminator_config = ElectraConfig.from_pretrained(discriminator_name)\n",
    "            elif self.args.model_name:\n",
    "                self.discriminator_config = ElectraConfig.from_pretrained(\n",
    "                    os.path.join(self.args.model_name, \"discriminator_config\"), **kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.discriminator_config = ElectraConfig(**self.args.discriminator_config, **kwargs)\n",
    "                if new_tokenizer:\n",
    "                    self.discriminator_config.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        if self.args.block_size <= 0:\n",
    "            self.args.block_size = min(self.args.max_seq_length, self.tokenizer.model_max_length)\n",
    "        else:\n",
    "            self.args.block_size = min(self.args.block_size, self.tokenizer.model_max_length, self.args.max_seq_length)\n",
    "\n",
    "        if self.args.model_name:\n",
    "            if self.args.model_type == \"electra\":\n",
    "                if self.args.model_name == \"electra\":\n",
    "                    generator_model = ElectraForMaskedLM.from_pretrained(generator_name)\n",
    "                    discriminator_model = ElectraForPreTraining.from_pretrained(discriminator_name)\n",
    "                    self.model = ElectraForLanguageModelingModel(\n",
    "                        config=self.config,\n",
    "                        generator_model=generator_model,\n",
    "                        discriminator_model=discriminator_model,\n",
    "                        generator_config=self.generator_config,\n",
    "                        discriminator_config=self.discriminator_config,\n",
    "                        tie_generator_and_discriminator_embeddings=self.args.tie_generator_and_discriminator_embeddings,\n",
    "                    )\n",
    "                    model_to_resize = (\n",
    "                        self.model.generator_model.module\n",
    "                        if hasattr(self.model.generator_model, \"module\")\n",
    "                        else self.model.generator_model\n",
    "                    )\n",
    "                    model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "                    model_to_resize = (\n",
    "                        self.model.discriminator_model.module\n",
    "                        if hasattr(self.model.discriminator_model, \"module\")\n",
    "                        else self.model.discriminator_model\n",
    "                    )\n",
    "                    model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "                    self.model.generator_model = generator_model\n",
    "                    self.model.discriminator_model = discriminator_model\n",
    "                else:\n",
    "                    self.model = model_class.from_pretrained(\n",
    "                        model_name,\n",
    "                        config=self.config,\n",
    "                        cache_dir=self.args.cache_dir,\n",
    "                        generator_config=self.generator_config,\n",
    "                        discriminator_config=self.discriminator_config,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "                    self.model.load_state_dict(\n",
    "                        torch.load(os.path.join(self.args.model_name, \"pytorch_model.bin\"), map_location=self.device)\n",
    "                    )\n",
    "            else:\n",
    "                self.model = model_class.from_pretrained(\n",
    "                    model_name, config=self.config, cache_dir=self.args.cache_dir, **kwargs,\n",
    "                )\n",
    "        else:\n",
    "            logger.info(\" Training language model from scratch\")\n",
    "            if self.args.model_type == \"electra\":\n",
    "                generator_model = ElectraForMaskedLM(config=self.generator_config)\n",
    "                discriminator_model = ElectraForPreTraining(config=self.discriminator_config)\n",
    "                self.model = ElectraForLanguageModelingModel(\n",
    "                    config=self.config,\n",
    "                    generator_model=generator_model,\n",
    "                    discriminator_model=discriminator_model,\n",
    "                    generator_config=self.generator_config,\n",
    "                    discriminator_config=self.discriminator_config,\n",
    "                    tie_generator_and_discriminator_embeddings=self.args.tie_generator_and_discriminator_embeddings,\n",
    "                )\n",
    "                model_to_resize = (\n",
    "                    self.model.generator_model.module\n",
    "                    if hasattr(self.model.generator_model, \"module\")\n",
    "                    else self.model.generator_model\n",
    "                )\n",
    "                model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "                model_to_resize = (\n",
    "                    self.model.discriminator_model.module\n",
    "                    if hasattr(self.model.discriminator_model, \"module\")\n",
    "                    else self.model.discriminator_model\n",
    "                )\n",
    "                model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "            else:\n",
    "                self.model = model_class(config=self.config)\n",
    "                model_to_resize = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "                model_to_resize.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        if model_type in [\"camembert\", \"xlmroberta\"]:\n",
    "            warnings.warn(\n",
    "                f\"use_multiprocessing automatically disabled as {model_type}\"\n",
    "                \" fails when using multiprocessing for feature conversion.\"\n",
    "            )\n",
    "            self.args.use_multiprocessing = False\n",
    "\n",
    "        if self.args.wandb_project and not wandb_available:\n",
    "            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n",
    "            self.args.wandb_project = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "\n",
    "default_vocab_path = (\n",
    "        pkg_resources.resource_filename(\n",
    "                    \"rxnfp\",\n",
    "                    \"models/transformers/bert_ft/vocab.txt\"\n",
    "                )\n",
    "    )\n",
    "\n",
    "model = SmilesLanguageModelingModel(model_type='bert', model_name=None, args={'vocab_path': default_vocab_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SmilesClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# optional\n",
    "from simpletransformers.config.model_args import ClassificationArgs\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "                                               \n",
    "from simpletransformers.classification.classification_model import (MODELS_WITHOUT_SLIDING_WINDOW_SUPPORT,\n",
    "                                               MODELS_WITHOUT_CLASS_WEIGHTS_SUPPORT,\n",
    "                                               MODELS_WITH_EXTRA_SEP_TOKEN, \n",
    "                                               MODELS_WITH_ADD_PREFIX_SPACE)\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "class SmilesClassificationModel(ClassificationModel):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type, \n",
    "        model_name,\n",
    "        tokenizer_type=None,\n",
    "        tokenizer_name=None,\n",
    "        num_labels=None, \n",
    "        weight=None,\n",
    "        args=None, \n",
    "        use_cuda=True, \n",
    "        cuda_device=-1,\n",
    "        onnx_execution_provider=None,\n",
    "        freeze_encoder=False, \n",
    "        freeze_all_but_one=False, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes a SmilesClassificationModel model.\n",
    "        \n",
    "        Main difference to https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/classification/classification_model.py\n",
    "        is that it uses a SmilesTokenizer instead of the original Tokenizer\n",
    "\n",
    "        Args:\n",
    "            model_type: The type of model (bert, xlnet, xlm, roberta, distilbert)\n",
    "            model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n",
    "            tokenizer_type: The type of tokenizer (auto, bert, xlnet, xlm, roberta, distilbert, etc.) to use. If a string is passed, Simple Transformers will try to initialize a tokenizer class from the available MODEL_CLASSES.\n",
    "                                Alternatively, a Tokenizer class (subclassed from PreTrainedTokenizer) can be passed.\n",
    "            tokenizer_name: The name/path to the tokenizer. If the tokenizer_type is not specified, the model_type will be used to determine the type of the tokenizer.\n",
    "            num_labels (optional): The number of labels or classes in the dataset.\n",
    "            weight (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            onnx_execution_provider (optional): ExecutionProvider to use with ONNX Runtime. Will use CUDA (if use_cuda) or CPU (if use_cuda is False) by default\n",
    "            freeze_encoder (optional): Do not train encoder (default: False).\n",
    "            freeze_all_but_one (optional): Only train last encoder layer (default: False).\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        MODEL_CLASSES = {\n",
    "            \"bert\": (BertConfig, BertForSequenceClassification, SmilesTokenizer),\n",
    "        }\n",
    "        \n",
    "        if model_type not in MODEL_CLASSES.keys():\n",
    "            raise NotImplementedException(f\"Currently the following model types are implemented: {MODEL_CLASSES.keys()}\")\n",
    "\n",
    "        self.args = self._load_model_args(model_name)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, ClassificationArgs):\n",
    "            self.args = args\n",
    "\n",
    "        if (\n",
    "            model_type in MODELS_WITHOUT_SLIDING_WINDOW_SUPPORT\n",
    "            and self.args.sliding_window\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"{} does not currently support sliding window\".format(model_type)\n",
    "            )\n",
    "\n",
    "        if self.args.thread_count:\n",
    "            torch.set_num_threads(self.args.thread_count)\n",
    "\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            self.is_sweeping = True\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = sweep_config_to_sweep_values(sweep_config)\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "        else:\n",
    "            self.is_sweeping = False\n",
    "\n",
    "        if self.args.manual_seed:\n",
    "            random.seed(self.args.manual_seed)\n",
    "            np.random.seed(self.args.manual_seed)\n",
    "            torch.manual_seed(self.args.manual_seed)\n",
    "            if self.args.n_gpu > 0:\n",
    "                torch.cuda.manual_seed_all(self.args.manual_seed)\n",
    "\n",
    "        if self.args.labels_list:\n",
    "            if num_labels:\n",
    "                assert num_labels == len(self.args.labels_list)\n",
    "            if self.args.labels_map:\n",
    "                try:\n",
    "                    assert list(self.args.labels_map.keys()) == self.args.labels_list\n",
    "                except AssertionError:\n",
    "                    assert [\n",
    "                        int(key) for key in list(self.args.labels_map.keys())\n",
    "                    ] == self.args.labels_list\n",
    "                    self.args.labels_map = {\n",
    "                        int(key): value for key, value in self.args.labels_map.items()\n",
    "                    }\n",
    "            else:\n",
    "                self.args.labels_map = {\n",
    "                    label: i for i, label in enumerate(self.args.labels_list)\n",
    "                }\n",
    "        else:\n",
    "            len_labels_list = 2 if not num_labels else num_labels\n",
    "            self.args.labels_list = [i for i in range(len_labels_list)]\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "        if tokenizer_type is not None:\n",
    "            if isinstance(tokenizer_type, str):\n",
    "                _, _, tokenizer_class = MODEL_CLASSES[tokenizer_type]\n",
    "            else:\n",
    "                tokenizer_class = tokenizer_type\n",
    "\n",
    "        if num_labels:\n",
    "            self.config = config_class.from_pretrained(\n",
    "                model_name, num_labels=num_labels, **self.args.config\n",
    "            )\n",
    "            self.num_labels = num_labels\n",
    "        else:\n",
    "            self.config = config_class.from_pretrained(model_name, **self.args.config)\n",
    "            self.num_labels = self.config.num_labels\n",
    "\n",
    "        if model_type in MODELS_WITHOUT_CLASS_WEIGHTS_SUPPORT and weight is not None:\n",
    "            raise ValueError(\n",
    "                \"{} does not currently support class weights\".format(model_type)\n",
    "            )\n",
    "        else:\n",
    "            self.weight = weight\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \" Make sure CUDA is available or set use_cuda=False.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        if self.args.onnx:\n",
    "            from onnxruntime import InferenceSession, SessionOptions\n",
    "\n",
    "            if not onnx_execution_provider:\n",
    "                onnx_execution_provider = (\n",
    "                    \"CUDAExecutionProvider\" if use_cuda else \"CPUExecutionProvider\"\n",
    "                )\n",
    "\n",
    "            options = SessionOptions()\n",
    "\n",
    "            if self.args.dynamic_quantize:\n",
    "                model_path = quantize(Path(os.path.join(model_name, \"onnx_model.onnx\")))\n",
    "                self.model = InferenceSession(\n",
    "                    model_path.as_posix(), options, providers=[onnx_execution_provider]\n",
    "                )\n",
    "            else:\n",
    "                model_path = os.path.join(model_name, \"onnx_model.onnx\")\n",
    "                self.model = InferenceSession(\n",
    "                    model_path, options, providers=[onnx_execution_provider]\n",
    "                )\n",
    "        else:\n",
    "            if not self.args.quantized_model:\n",
    "                if self.weight:\n",
    "                    self.model = model_class.from_pretrained(\n",
    "                        model_name,\n",
    "                        config=self.config,\n",
    "                        weight=torch.Tensor(self.weight).to(self.device),\n",
    "                        **kwargs,\n",
    "                    )\n",
    "                else:\n",
    "                    self.model = model_class.from_pretrained(\n",
    "                        model_name, config=self.config, **kwargs\n",
    "                    )\n",
    "            else:\n",
    "                quantized_weights = torch.load(\n",
    "                    os.path.join(model_name, \"pytorch_model.bin\")\n",
    "                )\n",
    "                if self.weight:\n",
    "                    self.model = model_class.from_pretrained(\n",
    "                        None,\n",
    "                        config=self.config,\n",
    "                        state_dict=quantized_weights,\n",
    "                        weight=torch.Tensor(self.weight).to(self.device),\n",
    "                    )\n",
    "                else:\n",
    "                    self.model = model_class.from_pretrained(\n",
    "                        None, config=self.config, state_dict=quantized_weights\n",
    "                    )\n",
    "\n",
    "            if self.args.dynamic_quantize:\n",
    "                self.model = torch.quantization.quantize_dynamic(\n",
    "                    self.model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "                )\n",
    "            if self.args.quantized_model:\n",
    "                self.model.load_state_dict(quantized_weights)\n",
    "            if self.args.dynamic_quantize:\n",
    "                self.args.quantized_model = True\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args.fp16 = False\n",
    "\n",
    "        if self.args.fp16:\n",
    "            try:\n",
    "                from torch.cuda import amp\n",
    "            except AttributeError:\n",
    "                raise AttributeError(\n",
    "                    \"fp16 requires Pytorch >= 1.6. Please update Pytorch or turn off fp16.\"\n",
    "                )\n",
    "\n",
    "        if tokenizer_name is None:\n",
    "            tokenizer_name = model_name\n",
    "\n",
    "        if tokenizer_name in [\n",
    "            \"vinai/bertweet-base\",\n",
    "            \"vinai/bertweet-covid19-base-cased\",\n",
    "            \"vinai/bertweet-covid19-base-uncased\",\n",
    "        ]:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                tokenizer_name,\n",
    "                do_lower_case=self.args.do_lower_case,\n",
    "                normalization=True,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                tokenizer_name, do_lower_case=self.args.do_lower_case, **kwargs\n",
    "            )\n",
    "\n",
    "        if self.args.special_tokens_list:\n",
    "            self.tokenizer.add_tokens(\n",
    "                self.args.special_tokens_list, special_tokens=True\n",
    "            )\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.args.model_name = model_name\n",
    "        self.args.model_type = model_type\n",
    "        self.args.tokenizer_name = tokenizer_name\n",
    "        self.args.tokenizer_type = tokenizer_type\n",
    "\n",
    "        if model_type in [\"camembert\", \"xlmroberta\"]:\n",
    "            warnings.warn(\n",
    "                f\"use_multiprocessing automatically disabled as {model_type}\"\n",
    "                \" fails when using multiprocessing for feature conversion.\"\n",
    "            )\n",
    "            self.args.use_multiprocessing = False\n",
    "\n",
    "        if self.args.wandb_project and not wandb_available:\n",
    "            warnings.warn(\n",
    "                \"wandb_project specified but wandb is not available. Wandb disabled.\"\n",
    "            )\n",
    "            self.args.wandb_project = None\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'classifier' in name:\n",
    "                    continue\n",
    "                param.requires_grad = False\n",
    "        elif freeze_all_but_one:\n",
    "            n_layers = self.model.config.num_hidden_layers\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if str(n_layers-1) in name:\n",
    "                    continue\n",
    "                elif 'classifier' in name:\n",
    "                    continue\n",
    "                elif 'pooler' in name:\n",
    "                    continue\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/phs/git/post/update/rxnfp/rxnfp/models/transformers/bert_pretrained were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/phs/git/post/update/rxnfp/rxnfp/models/transformers/bert_pretrained and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# optional\n",
    "default_model_path = (\n",
    "        pkg_resources.resource_filename(\n",
    "                    \"rxnfp\",\n",
    "                    \"models/transformers/bert_pretrained\"\n",
    "                )\n",
    "    )\n",
    "model_args = {\n",
    "     'num_train_epochs': 10, 'overwrite_output_dir': True,\n",
    "    'learning_rate': 0.0001, 'gradient_accumulation_steps': 1,\n",
    "    'regression': True, \"num_labels\":1, \"fp16\": False,\n",
    "    \"evaluate_during_training\": True, 'manual_seed': 42,\n",
    "    \"max_seq_length\": 300, \"train_batch_size\": 16,\"warmup_ratio\": 0.00,\n",
    "    \"config\" : { 'hidden_dropout_prob': 0.05 },\n",
    "    # 'wandb_project': 'test_project', \n",
    "}\n",
    "model = SmilesClassificationModel(\"bert\", default_model_path, num_labels=5, \n",
    "                                       args=model_args, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
